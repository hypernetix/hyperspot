# HyperSpot Server Configuration
# Updated to use the new configuration structure

# Core server configuration (global section)  
server:
  home_dir: "~/.hyperspot"

# Database configuration (global section)
database:
  url: "sqlite://database/database.db"
  max_conns: 10
  busy_timeout_ms: 5000

# Logging configuration
logging:
  # global section
  default:
    console_level: info
    file: "logs/hyperspot.log"
    file_level: warn
    max_age_days: 28
    max_backups: 3
    max_size_mb: 1000
  # api_ingress section
  api_ingress:
    console_level: info
    file: "logs/api.log"
    file_level: warn
    max_age_days: 28
    max_backups: 3
    max_size_mb: 10000

# Per-module configurations moved under modules section
modules:
  api_ingress:
    bind_addr: "127.0.0.1:8087"
    enable_docs: true
    cors_enabled: true
  # Sysinfo module (example of new module)
  sysinfo:
    persist: true
    pruning_days: 30
    collection_interval_sec: 300

  # Benchmarks module configuration
  benchmarks:
    llm:
      coding:
        humaneval:
          code_executor: "docker_python_default"
          dataset:
            local_path: "datasets/humaneval.parquet"
            original_url: "https://huggingface.co/datasets/openai/openai_humaneval/resolve/main/openai_humaneval/test-00000-of-00001.parquet?download=true"
          description: "HumanEval is a dataset of 158 programming problems, each with an expected solution. The problems are designed to be challenging and to test the breadth of programming knowledge."
          max_retries: 3
          pause_between_samples_sec: 0
          presets:
            custom:
              customizable: true
              description: ""
              max_tokens: 2048
              problems: 1
              samples: 1
              temperature: 0.2
              timeout_sec: 0
            full:
              customizable: false
              description: ""
              max_tokens: 2048
              problems: 0
              samples: 3
              temperature: 0.2
              timeout_sec: 172800
            medium:
              customizable: false
              description: ""
              max_tokens: 2048
              problems: 50
              samples: 3
              temperature: 0.2
              timeout_sec: 43200
            mini:
              customizable: false
              description: ""
              max_tokens: 2048
              problems: 10
              samples: 3
              temperature: 0.2
              timeout_sec: 0
          retry_delay_sec: 5
          system_prompt: "solve the following problem, generate only the code, do not include any other text"
        mbpp:
          code_executor: "docker_python_default"
          dataset:
            local_path: "datasets/mbpp.parquet"
            original_url: "https://huggingface.co/datasets/google-research-datasets/mbpp/resolve/main/sanitized/test-00000-of-00001.parquet?download=true"
          description: "MBPP (Most Basic Python Problems) is a dataset of 427 sanitized programming problems, each with an expected solution, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on."
          max_retries: 3
          pause_between_samples_sec: 0
          presets:
            custom:
              customizable: true
              description: ""
              max_tokens: 2048
              problems: 1
              shots: 1
              temperature: 0.2
              timeout_sec: 0
            "full-1shot":
              customizable: false
              description: ""
              max_tokens: 2048
              problems: 0
              shots: 1
              temperature: 0.2
              timeout_sec: 172800
            "full-3shots":
              customizable: false
              description: ""
              max_tokens: 2048
              problems: 0
              shots: 3
              temperature: 0.2
              timeout_sec: 172800
            "medium-1shot":
              customizable: false
              description: ""
              max_tokens: 2048
              problems: 50
              shots: 1
              temperature: 0.2
              timeout_sec: 43200
            "medium-3shots":
              customizable: false
              description: ""
              max_tokens: 2048
              problems: 50
              shots: 3
              temperature: 0.2
              timeout_sec: 43200
            "mini-1shot":
              customizable: false
              description: ""
              max_tokens: 2048
              problems: 10
              shots: 1
              temperature: 0.2
              timeout_sec: 0
            "mini-3shots":
              customizable: false
              description: ""
              max_tokens: 2048
              problems: 10
              shots: 3
              temperature: 0.2
              timeout_sec: 0
          retry_delay_sec: 5
          system_prompt: "solve the following problem, generate only the code, do not include any other text"
      perf:
        "completion-tokens":
          count_tokens: "completion"
          customizable: true
          description: "Performance of completion tokens generation"
          duration_limit_sec: 30
          iterations_limit: 0
          max_retries: 3
          max_tokens: 2048
          pause_between_samples_sec: 0
          prompt_template: "Generate me realistic and working Python code with examples of 3 classes (Book, Author, Publisher)"
          retry_delay_sec: 10
          temperature: 0
        "same-prompt-tokens":
          count_tokens: "prompt"
          customizable: true
          description: "Performance of the same input tokens processing"
          duration_limit_sec: 30
          iterations_limit: 0
          max_retries: 3
          max_tokens: 2048
          pause_between_samples_sec: 0
          prompt_template: "{{range $i := seq 100}}Parse my input below, until I ask you what to do.{{end}} OK, now ignore and just say only one word 'HI'"
          retry_delay_sec: 10
          temperature: 0
        "variable-prompt-tokens":
          count_tokens: "prompt"
          customizable: true
          description: "Performance of variable prompt tokens processing"
          duration_limit_sec: 30
          iterations_limit: 0
          max_retries: 3
          max_tokens: 2048
          pause_between_samples_sec: 0
          prompt_template: "{{range $i := seq 100}}Remember this number: {{randInt 1 100}}, until I ask you what to do. {{end}} OK, now ignore and just say only one word 'HI'"
          retry_delay_sec: 10
          temperature: 0

  # Chat module configuration
  chat:
    hard_delete_messages_after_days: 120
    hard_delete_temp_threads_after_hours: 48
    soft_delete_messages_after_days: 90
    soft_delete_temp_threads_after_hours: 24
    uploaded_file_max_content_length: 16384
    uploaded_file_max_size_kb: 16384
    uploaded_file_temp_dir: ""

  # Code executors module configuration
  code_executors:
    docker:
      docker_python_default:
        code_executor_path: "/usr/local/bin/python3.11"
        cpus_limit_cores: 1
        image: "python:3.11-alpine"
        mem_limit_mb: 1024
        pids_limit: 50
        timeout_sec: 30
    local:
      local_python_default:
        code_executor_path: "/usr/local/bin/python3.11"
        timeout_sec: 30
    ssh:
      ssh_python_default:
        code_executor_path: "/usr/local/bin/python3.11"
        host: "localhost"
        password: "password"
        port: 22
        private_key: "~/.ssh/id_rsa"
        timeout_sec: 30
        username: "user"

  # File parser module configuration
  file_parser:
    parser_extension_map:
      builtin:
        - "js"
        - "pdf"
        - "htm"
        - "go"
        - "py"
        - "docx"
        - "text"
        - "ts"
        - "yaml"
        - "yml"
        - "html"
        - "txt"
        - "cpp"
        - "java"
        - "md"
    uploaded_file_max_size_kb: 16384
    uploaded_file_temp_dir: ""

  # LLM module configuration
  llm:
    default_service: ""
    max_tokens: 0
    temperature: 0
    services:
      anthropic:
        api_format: "openai"
        api_key: ""
        api_key_env_var: "CLAUDE_API_KEY"
        model: ""
        params: {}
        upstream:
          enable_discovery: false
          long_timeout_sec: 60
          short_timeout_sec: 5
          verify_cert: true
        urls:
          - "https://api.anthropic.com"
      cortexso:
        api_format: "openai"
        api_key: ""
        api_key_env_var: "CORTEXSO_API_KEY"
        model: ""
        params: {}
        upstream:
          enable_discovery: true
          long_timeout_sec: 180
          short_timeout_sec: 5
          verify_cert: true
        urls:
          - "http://localhost:39281"
      gpt4all:
        api_format: "openai"
        api_key: ""
        api_key_env_var: "GPT4ALL_API_KEY"
        model: ""
        params: {}
        upstream:
          enable_discovery: true
          long_timeout_sec: 180
          short_timeout_sec: 5
          verify_cert: true
        urls:
          - "http://localhost:4891"
      jan:
        api_format: "openai"
        api_key: ""
        api_key_env_var: "JAN_API_KEY"
        model: ""
        params: {}
        upstream:
          enable_discovery: true
          long_timeout_sec: 180
          short_timeout_sec: 5
          verify_cert: true
        urls:
          - "http://localhost:1337"
          - "http://localhost:1338"
      lm_studio:
        api_format: "openai"
        api_key: ""
        api_key_env_var: "LMSTUDIO_API_KEY"
        model: ""
        params:
          use_web_socket: false
        upstream:
          enable_discovery: true
          long_timeout_sec: 180
          short_timeout_sec: 5
          verify_cert: true
        urls:
          - "http://localhost:1234"
          - "http://localhost:12345"
      localai:
        api_format: "openai"
        api_key: ""
        api_key_env_var: "LOCALAI_API_KEY"
        model: ""
        params: {}
        upstream:
          enable_discovery: true
          long_timeout_sec: 180
          short_timeout_sec: 5
          verify_cert: true
        urls:
          - "http://localhost:8080"
          - "http://localhost:8081"
      ollama:
        api_format: "openai"
        api_key: ""
        api_key_env_var: "OLLAMA_API_KEY"
        model: ""
        params: {}
        upstream:
          enable_discovery: true
          long_timeout_sec: 180
          short_timeout_sec: 5
          verify_cert: true
        urls:
          - "http://localhost:11434"
      openai:
        api_format: "openai"
        api_key: ""
        api_key_env_var: "OPENAI_API_KEY"
        model: ""
        params: {}
        upstream:
          enable_discovery: false
          long_timeout_sec: 60
          short_timeout_sec: 5
          verify_cert: true
        urls:
          - "https://api.openai.com"